{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a02285e6","cell_type":"markdown","source":"# Starter Notebook","metadata":{"id":"a02285e6"}},{"id":"bdcc5329","cell_type":"markdown","source":"Install and import required libraries","metadata":{"id":"bdcc5329"}},{"id":"348ceed6-b684-46c3-8a32-9bb640c9a9d7","cell_type":"code","source":"!pip install transformers datasets evaluate accelerate peft trl bitsandbytes\n!pip install nvidia-ml-py3\n!pip install langdetect","metadata":{"id":"348ceed6-b684-46c3-8a32-9bb640c9a9d7","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:04:32.053466Z","iopub.execute_input":"2025-04-21T18:04:32.053662Z","iopub.status.idle":"2025-04-21T18:06:11.721136Z","shell.execute_reply.started":"2025-04-21T18:04:32.053644Z","shell.execute_reply":"2025-04-21T18:06:11.720412Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nCollecting trl\n  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (14.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.16.1-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, trl, evaluate, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 evaluate-0.4.3 fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.16.1\nCollecting nvidia-ml-py3\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: nvidia-ml-py3\n  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19173 sha256=22e473c170bd2f412468e13153e6062205e92c2917916ee2c968192b308ee205\n  Stored in directory: /root/.cache/pip/wheels/47/50/9e/29dc79037d74c3c1bb4a8661fb608e8674b7e4260d6a3f8f51\nSuccessfully built nvidia-ml-py3\nInstalling collected packages: nvidia-ml-py3\nSuccessfully installed nvidia-ml-py3-7.352.0\nCollecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=c50a55217df8eb269fe0574ee04283aca549b6be48f386af0f457f0e8fd2faca\n  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\n","output_type":"stream"}],"execution_count":1},{"id":"8e65013c-f2b5-40f6-9311-2fe0fb4e0c48","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:06:11.722958Z","iopub.execute_input":"2025-04-21T18:06:11.723171Z","iopub.status.idle":"2025-04-21T18:06:11.999711Z","shell.execute_reply.started":"2025-04-21T18:06:11.723152Z","shell.execute_reply":"2025-04-21T18:06:11.999078Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\n","output_type":"stream"}],"execution_count":2},{"id":"cca64f38-d8d2-4313-8295-fbbd43c2a263","cell_type":"code","source":"import torch\nfrom transformers import RobertaModel, RobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification\nfrom peft import LoraConfig, get_peft_model, PeftModel\nfrom datasets import load_dataset, Dataset, ClassLabel\nimport pickle\nfrom langdetect import detect\nfrom langdetect.lang_detect_exception import LangDetectException\nfrom transformers import pipeline\nimport re\nimport nltk\nfrom nltk.corpus import stopwords","metadata":{"id":"cca64f38-d8d2-4313-8295-fbbd43c2a263","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:06:12.000371Z","iopub.execute_input":"2025-04-21T18:06:12.000829Z","iopub.status.idle":"2025-04-21T18:06:41.540029Z","shell.execute_reply.started":"2025-04-21T18:06:12.000807Z","shell.execute_reply":"2025-04-21T18:06:41.539424Z"}},"outputs":[{"name":"stderr","text":"2025-04-21 18:06:25.969713: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745258786.259329      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745258786.334158      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"id":"59d6e377","cell_type":"markdown","source":"## Load Tokenizer and Preprocess Data","metadata":{"id":"59d6e377"}},{"id":"21f42747-f551-40a5-a95f-7affb1eba4a3","cell_type":"code","source":"# Augment training dataset using contextual_augment\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\n\n# Initialize with constants\nbase_model = 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(base_model)\nstop_words = set(stopwords.words('english'))\n\n# Text cleaning function\ndef clean_text(text):\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'[^\\w\\s.,!?]', '', text)\n    return ' '.join(text.split()).strip()\n\n# Enhanced preprocessing\ndef preprocess(examples):\n    cleaned_texts = [clean_text(text) for text in examples['text']]\n    return tokenizer(cleaned_texts, padding='max_length', truncation=True, max_length=128)\n\n# Perform Filtering\ndef filter_fn(example):\n    try:\n        # Clean the input text (assumes clean_text handles unwanted characters, HTML, etc.)\n        text = clean_text(example['text'])\n\n        # Skip if the cleaned text is empty\n        if not text:\n            return False\n\n        # Tokenize the text into words and filter out too short or too long samples\n        words = text.split()\n        if len(words) < 5 or len(words) > 200:\n            return False\n\n        # Compute the ratio of alphabetic characters to total characters (ignore very noisy text)\n        alpha_ratio = sum(c.isalpha() for c in text) / max(1, len(text))\n        if alpha_ratio < 0.6:\n            return False\n\n        # Check if the language is English using langdetect\n        if detect(text) != 'en':\n            return False\n            \n        # Tokenize the text and remove stopwords\n        word_tokens = nltk.word_tokenize(text.lower())\n\n        # Ensure enough meaningful content is present (at least 30% non-stopwords)\n        meaningful_words = [w for w in word_tokens if w not in stop_words]\n        if len(meaningful_words) / max(1, len(word_tokens)) < 0.3:\n            return False\n            \n        return True\n    except:\n        return False\n\n# Load and process dataset\ndataset = load_dataset('ag_news', split='train')\nfiltered_dataset = dataset.filter(filter_fn)\ntokenized_dataset = filtered_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\ntokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")","metadata":{"id":"21f42747-f551-40a5-a95f-7affb1eba4a3","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:06:41.540758Z","iopub.execute_input":"2025-04-21T18:06:41.541327Z","iopub.status.idle":"2025-04-21T18:12:36.566343Z","shell.execute_reply.started":"2025-04-21T18:06:41.541309Z","shell.execute_reply":"2025-04-21T18:12:36.565635Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b282505a07d4ab9ac9682e249d01a4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f747250b58240ff9a931b9568b65a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"416f58e225a1438aa32dff8fb3af8279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b43df9b660974b49b4b955892f9f049f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4198e8992664481a335638f3ac0c556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2e64467e419485ebb6d727e0913a5e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7279e260f9744858819b550072f8c784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61202b8a7ec74dc385e37d2fb23501a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a0554a5dc2340d78f53a8507b5890be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"675505b442254e3f993176be1ae48a69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dec44e89fed8417e9fa285107bcce98e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/119948 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e82c1e6203446b80bafa9faa9a6a0b"}},"metadata":{}}],"execution_count":4},{"id":"9e07f641-bec0-43a6-8c26-510d7642916a","cell_type":"code","source":"# Extract the number of classess and their names\nnum_labels = dataset.features['label'].num_classes\nclass_names = dataset.features[\"label\"].names\nprint(f\"number of labels: {num_labels}\")\nprint(f\"the labels: {class_names}\")\n\n# Create an id2label mapping\n# We will need this for our classifier.\nid2label = {i: label for i, label in enumerate(class_names)}\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")","metadata":{"id":"9e07f641-bec0-43a6-8c26-510d7642916a","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:12:36.567250Z","iopub.execute_input":"2025-04-21T18:12:36.567499Z","iopub.status.idle":"2025-04-21T18:12:36.572139Z","shell.execute_reply.started":"2025-04-21T18:12:36.567482Z","shell.execute_reply":"2025-04-21T18:12:36.571418Z"}},"outputs":[{"name":"stdout","text":"number of labels: 4\nthe labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n","output_type":"stream"}],"execution_count":5},{"id":"29876d2d-5e21-44cb-b26c-9af3c010a2c0","cell_type":"markdown","source":"## Load the Test Data","metadata":{}},{"id":"69fe055c-7ced-4314-a49d-650f7392e51e","cell_type":"code","source":"import pickle\n\ntest_directory = '/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl'\n# Load the test dataset from .pkl file\nwith open(test_directory, 'rb') as f:\n    test_dataset = pickle.load(f)\n\ntokenized_test = test_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n\nprint(tokenized_test)\nprint(type(tokenized_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:12:36.572726Z","iopub.execute_input":"2025-04-21T18:12:36.572882Z","iopub.status.idle":"2025-04-21T18:12:42.520861Z","shell.execute_reply.started":"2025-04-21T18:12:36.572869Z","shell.execute_reply":"2025-04-21T18:12:42.520289Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9966610299e643079bdca5c09c8ace80"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 8000\n})\n<class 'datasets.arrow_dataset.Dataset'>\n","output_type":"stream"}],"execution_count":6},{"id":"c9e24afd","cell_type":"markdown","source":"## Load Pre-trained Model\nSet up config for pretrained model and download it from hugging face","metadata":{"id":"c9e24afd"}},{"id":"262a8416-a59c-4ea1-95d9-0b1f81d6094c","cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained(\n    base_model,\n    num_labels=num_labels,\n    id2label=id2label)\nmodel","metadata":{"id":"262a8416-a59c-4ea1-95d9-0b1f81d6094c","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:12:42.522725Z","iopub.execute_input":"2025-04-21T18:12:42.522941Z","iopub.status.idle":"2025-04-21T18:12:45.382659Z","shell.execute_reply.started":"2025-04-21T18:12:42.522924Z","shell.execute_reply":"2025-04-21T18:12:45.382077Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d238bea7122a486eb3dd4d2d3910fe24"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":7},{"id":"e7413430-be57-482b-856e-36bd4ba799df","cell_type":"code","source":"# Split the original training set\nsplit_datasets = tokenized_dataset.train_test_split(test_size=640, seed=42)\ntrain_dataset = split_datasets['train']\neval_dataset = split_datasets['test']","metadata":{"id":"e7413430-be57-482b-856e-36bd4ba799df","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:12:45.383331Z","iopub.execute_input":"2025-04-21T18:12:45.383583Z","iopub.status.idle":"2025-04-21T18:12:45.417680Z","shell.execute_reply.started":"2025-04-21T18:12:45.383555Z","shell.execute_reply":"2025-04-21T18:12:45.417173Z"}},"outputs":[],"execution_count":8},{"id":"652452e3","cell_type":"markdown","source":"## Setup LoRA Config\nSetup PEFT config and get peft model for finetuning","metadata":{"id":"652452e3"}},{"id":"bd0ca0ea-86b8-47f7-8cbf-83da25685876","cell_type":"code","source":"# PEFT Config\npeft_config = LoraConfig(\n    r=9,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias = 'none',\n    target_modules = ['query', 'value'],\n    task_type=\"SEQ_CLS\",\n)","metadata":{"id":"bd0ca0ea-86b8-47f7-8cbf-83da25685876","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:12:45.418340Z","iopub.execute_input":"2025-04-21T18:12:45.418538Z","iopub.status.idle":"2025-04-21T18:12:45.422321Z","shell.execute_reply.started":"2025-04-21T18:12:45.418517Z","shell.execute_reply":"2025-04-21T18:12:45.421639Z"}},"outputs":[],"execution_count":9},{"id":"6ec2739d-76b6-4fde-91c2-0fc49e1884b0","cell_type":"code","source":"peft_model = get_peft_model(model, peft_config)\npeft_model","metadata":{"id":"6ec2739d-76b6-4fde-91c2-0fc49e1884b0","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:12:45.422955Z","iopub.execute_input":"2025-04-21T18:12:45.423138Z","iopub.status.idle":"2025-04-21T18:12:45.534820Z","shell.execute_reply.started":"2025-04-21T18:12:45.423124Z","shell.execute_reply":"2025-04-21T18:12:45.534179Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=9, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=9, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=9, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=9, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":10},{"id":"da45f85c-b016-4c49-8808-6eafa7cb5d1b","cell_type":"code","source":"print('PEFT Model')\npeft_model.print_trainable_parameters()","metadata":{"id":"da45f85c-b016-4c49-8808-6eafa7cb5d1b","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:12:45.535532Z","iopub.execute_input":"2025-04-21T18:12:45.535811Z","iopub.status.idle":"2025-04-21T18:12:45.540687Z","shell.execute_reply.started":"2025-04-21T18:12:45.535787Z","shell.execute_reply":"2025-04-21T18:12:45.540105Z"}},"outputs":[{"name":"stdout","text":"PEFT Model\ntrainable params: 925,444 || all params: 125,574,152 || trainable%: 0.7370\n","output_type":"stream"}],"execution_count":11},{"id":"6ba8de62-abfe-4c6a-8b33-3b1640ee1df9","cell_type":"markdown","source":"We can see above we have less than 1M parameters","metadata":{}},{"id":"12284b58","cell_type":"markdown","source":"## Training Setup","metadata":{"id":"12284b58"}},{"id":"0ee64c43-fe38-479a-b3c5-7d939a3db4c1","cell_type":"code","source":"# To track evaluation accuracy during training\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    accuracy = accuracy_score(labels, preds)\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    f1 = f1_score(labels, preds, average='weighted')\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }","metadata":{"id":"0ee64c43-fe38-479a-b3c5-7d939a3db4c1","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:12:45.541323Z","iopub.execute_input":"2025-04-21T18:12:45.541502Z","iopub.status.idle":"2025-04-21T18:12:45.559120Z","shell.execute_reply.started":"2025-04-21T18:12:45.541488Z","shell.execute_reply":"2025-04-21T18:12:45.558377Z"}},"outputs":[],"execution_count":12},{"id":"49fcf82f-4828-44c5-964e-352a55def109","cell_type":"code","source":"# Check if CUDA is available\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:12:45.559921Z","iopub.execute_input":"2025-04-21T18:12:45.560181Z","iopub.status.idle":"2025-04-21T18:12:45.574112Z","shell.execute_reply.started":"2025-04-21T18:12:45.560133Z","shell.execute_reply":"2025-04-21T18:12:45.573604Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nCUDA device count: 2\nCUDA device name: Tesla T4\n","output_type":"stream"}],"execution_count":13},{"id":"768b4917-65de-4e55-ae7f-698e287535d4","cell_type":"code","source":"# Setup Training args\noutput_dir = \"results\"\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    eval_strategy='epoch',\n    learning_rate=3e-4,\n    num_train_epochs=1,\n    save_strategy=\"no\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"cosine\",\n    dataloader_num_workers=4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim=\"adamw_torch\",\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True},\n    report_to=\"none\"\n)\n\ndef get_trainer(model):\n      return  Trainer(\n          model=model,\n          args=training_args,\n          compute_metrics=compute_metrics,\n          train_dataset=train_dataset,\n          eval_dataset=eval_dataset,\n          data_collator=data_collator,\n      )","metadata":{"id":"768b4917-65de-4e55-ae7f-698e287535d4","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:12:45.574735Z","iopub.execute_input":"2025-04-21T18:12:45.574937Z","iopub.status.idle":"2025-04-21T18:12:45.623353Z","shell.execute_reply.started":"2025-04-21T18:12:45.574923Z","shell.execute_reply":"2025-04-21T18:12:45.622845Z"}},"outputs":[],"execution_count":14},{"id":"9b848278","cell_type":"markdown","source":"### Start Training","metadata":{"id":"9b848278"}},{"id":"98d9d57d-b57f-4acc-80fb-fc5443e75515","cell_type":"code","source":"peft_lora_finetuning_trainer = get_trainer(peft_model)\n\nresult = peft_lora_finetuning_trainer.train()","metadata":{"id":"98d9d57d-b57f-4acc-80fb-fc5443e75515","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:12:45.624010Z","iopub.execute_input":"2025-04-21T18:12:45.624551Z","iopub.status.idle":"2025-04-21T18:14:18.901573Z","shell.execute_reply.started":"2025-04-21T18:12:45.624534Z","shell.execute_reply":"2025-04-21T18:14:18.900518Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='279' max='3729' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 279/3729 01:29 < 18:28, 3.11 it/s, Epoch 0.07/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/589319088.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpeft_lora_finetuning_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft_lora_finetuning_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2563\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2565\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2566\u001b[0m                     ):\n\u001b[1;32m   2567\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":15},{"id":"5183be7e-514f-4e64-a6f4-314a827e6be5","cell_type":"markdown","source":"## Evaluate Finetuned Model\n### Run Inference on eval_dataset","metadata":{"id":"5183be7e-514f-4e64-a6f4-314a827e6be5"}},{"id":"feefd1c6-a0be-4d6d-972f-a2f6e4c5b4aa","cell_type":"code","source":"# Evaluate on the validation set\n\neval_results = peft_lora_finetuning_trainer.evaluate()\nprint(f\"Evaluation results: {eval_results}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:14:18.902168Z","iopub.status.idle":"2025-04-21T18:14:18.902495Z","shell.execute_reply.started":"2025-04-21T18:14:18.902327Z","shell.execute_reply":"2025-04-21T18:14:18.902343Z"}},"outputs":[],"execution_count":null},{"id":"ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5","cell_type":"code","source":"from torch.utils.data import DataLoader\nimport evaluate\nfrom tqdm import tqdm\n\ndef evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n    \"\"\"\n    Evaluate a PEFT model on a dataset.\n\n    Args:\n        inference_model: The model to evaluate.\n        dataset: The dataset (Hugging Face Dataset) to run inference on.\n        labelled (bool): If True, the dataset includes labels and metrics will be computed.\n                         If False, only predictions will be returned.\n        batch_size (int): Batch size for inference.\n        data_collator: Function to collate batches. If None, the default collate_fn is used.\n\n    Returns:\n        If labelled is True, returns a tuple (metrics, predictions)\n        If labelled is False, returns the predictions.\n    \"\"\"\n    # Create the DataLoader\n    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    inference_model.to(device)\n    inference_model.eval()\n\n    all_predictions = []\n    if labelled:\n        metric = evaluate.load('accuracy')\n\n    # Loop over the DataLoader\n    for batch in tqdm(eval_dataloader):\n        # Move each tensor in the batch to the device\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = inference_model(**batch)\n        predictions = outputs.logits.argmax(dim=-1)\n        all_predictions.append(predictions.cpu())\n\n        if labelled:\n            # Expecting that labels are provided under the \"labels\" key.\n            references = batch[\"labels\"]\n            metric.add_batch(\n                predictions=predictions.cpu().numpy(),\n                references=references.cpu().numpy()\n            )\n\n    # Concatenate predictions from all batches\n    all_predictions = torch.cat(all_predictions, dim=0)\n\n    if labelled:\n        eval_metric = metric.compute()\n        print(\"Evaluation Metric:\", eval_metric)\n        return eval_metric, all_predictions\n    else:\n        return all_predictions","metadata":{"id":"ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:14:18.903651Z","iopub.status.idle":"2025-04-21T18:14:18.904256Z","shell.execute_reply.started":"2025-04-21T18:14:18.904074Z","shell.execute_reply":"2025-04-21T18:14:18.904091Z"}},"outputs":[],"execution_count":null},{"id":"75f39087-f2bb-49d3-9fe1-0d812fb30203","cell_type":"markdown","source":"### Run Inference on unlabelled dataset","metadata":{"id":"75f39087-f2bb-49d3-9fe1-0d812fb30203"}},{"id":"e60991d3-38b1-4657-8854-408ce66f6b84","cell_type":"code","source":"# Run inference and save predictions\npreds = evaluate_model(peft_model, tokenized_test, False, 8, data_collator)\ndf_output = pd.DataFrame({\n    'ID': range(len(preds)),\n    'Label': preds.numpy()  # or preds.tolist()\n})\ndf_output.to_csv(os.path.join(output_dir,\"submission.csv\"), index=False)\nprint(\"Inference complete. Predictions saved to submission.csv\")","metadata":{"id":"e60991d3-38b1-4657-8854-408ce66f6b84","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:14:18.905563Z","iopub.status.idle":"2025-04-21T18:14:18.905847Z","shell.execute_reply.started":"2025-04-21T18:14:18.905728Z","shell.execute_reply":"2025-04-21T18:14:18.905741Z"}},"outputs":[],"execution_count":null}]}